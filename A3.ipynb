{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Need this to access files from google drive\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive/2023\\ Sem1/COMPSCI\\ 361/361\\ Group\\ Work')"],"metadata":{"id":"YQZ4Rt-lsy-O","colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"status":"error","timestamp":1685099112077,"user_tz":-720,"elapsed":344,"user":{"displayName":"Yathi Kidambi","userId":"14071728732360383349"}},"outputId":"f7bcc2cc-43ee-4586-e623-1c9930e9c1a0"},"execution_count":3,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-62c9bc098922>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/2023\\ Sem1/COMPSCI\\ 361/361\\ Group\\ Work'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmountpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not contain a space.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   metadata_server_addr = (\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not contain a space."]}]},{"cell_type":"markdown","source":["# COMPSCI 361: Machine Learning\n","# Assignment 3: Group Project\n"],"metadata":{"id":"8WIbgbqH9O4D"}},{"cell_type":"markdown","source":["# **Task 1: Exploratory Data Analytics**"],"metadata":{"id":"pukBJeN89dn9"}},{"cell_type":"code","source":["\"\"\"\n"," I think this part and the vectorization should only happen once in Task 1.\n"," It's mentioned in the pdf instructions.\n","\"\"\" \n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# Load the dataset\n","data = pd.read_csv('train.csv')\n","data.head()"],"metadata":{"id":"rR2gMFV49yzP","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"error","timestamp":1685098354806,"user_tz":-720,"elapsed":2568,"user":{"displayName":"Yathi Kidambi","userId":"14071728732360383349"}},"outputId":"53d03367-374c-4a3b-b7c9-cc83bdf72b59"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b3e7ae500fc0>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}]},{"cell_type":"markdown","source":["# **Task 2: Classification Models Learning**"],"metadata":{"id":"EsL6JT679ffJ"}},{"cell_type":"markdown","source":["## **Part a - NB:**\n","Train a Naive Bayes classifier using all articles features. Report the \\\n","(i) top-20 most identifiable words that are most likely to occur in the articles over two classes using your NB classifier and  \\\n","(ii) the top-20 words that maximize the following quantity: \\\n","ùë∑(ùëø_ùíò = ùüè|ùíÄ = ùíö) / ùë∑(ùëø_ùíò = ùüè|ùíÄ ‚â† ùíö) \\\n","Which list of words describe the two classes better? Briefly explain your reasoning."],"metadata":{"id":"rxmfnxqb9fu2"}},{"cell_type":"code","source":["# Create the feature matrix using TF-IDF vectorization - encode document\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(data['Text'])\n","\n","# encode document\n","vector = vectorizer.transform(data['Text'])\n","\n","print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n","# summarize encoded vector\n","print(f'vector shape: {vector.shape}\\n')\n","print(f'article vector\\n {vector.toarray()}')"],"metadata":{"id":"YkWRn_vN90lI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature Extraction (Vectorization) using Term Frequency - Inverse Document Frequency (TF-IDF)"],"metadata":{"id":"5tNprQ4397cY"}},{"cell_type":"code","source":["# Create the target variable\n","y = data['Category']\n","\n","# Train the Naive Bayes classifier\n","classifier = MultinomialNB()\n","classifier.fit(X, y)\n","\n","# Get the feature names\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Top-20 Most Identifiable Words\n","class_labels = classifier.classes_\n","top_identifiable_words = []\n","\n","for i, class_label in enumerate(class_labels):\n","    class_prob_sorted = classifier.feature_log_prob_[i, :].argsort()\n","    top_words = [feature_names[word_index] for word_index in class_prob_sorted[:-21:-1]]\n","    top_identifiable_words.append(top_words)\n","\n","print(\"Top-20 Most Identifiable Words:\")\n","for i, class_label in enumerate(class_labels):\n","    print(f\"Class: {class_label}\")\n","    print(', '.join(top_identifiable_words[i]))\n","\n","# Top-20 Words Maximizing the Quantity\n","word_quantity = {}\n","\n","print(\"\\nTop-20 Words Maximizing the Quantity:\")\n","for i, class_label in enumerate(class_labels):\n","    for word_index in range(len(feature_names)):\n","        quantity = classifier.feature_log_prob_[i, word_index] - classifier.feature_log_prob_[(i+1) % len(class_labels), word_index]\n","        word_quantity[feature_names[word_index]] = quantity\n","\n","    top_quantity_words = sorted(word_quantity, key=word_quantity.get, reverse=True)[:20]\n","    print(f\"Class: {class_label}\")\n","    print(', '.join(top_quantity_words))"],"metadata":{"id":"L8ZRalvK900m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.a(i)**\n","The top-20 most identifiable words that are most likely to occur in the articles over two classes using our NB classifier are:\n","\\\n","Class: entertainment \n",">    film, best, said, show, band, music, year, awards, us, award, actor, album, star, chart, tv, also, number, oscar, top, new \n","\n","Class: tech \n",">   said, people, mobile, software, games, phone, net, users, technology, mr, microsoft, virus, computer, broadband, new, use, could, would, digital, game \\\n","\n","\n","**2.a(ii)**\n","The top-20 words that maximize the following quantity, ùë∑(ùëø_ùíò = ùüè|ùíÄ = ùíö) / ùë∑(ùëø_ùíò = ùüè|ùíÄ ‚â† ùíö), are: \n","\n","Class: entertainment\n",">    film, band, best, actor, album, chart, oscar, singer, award, actress, star, musical, stars, festival, comedy, awards, aviator, theatre, rock, nominated\n","\n","Class: tech\n",">    mobile, software, users, microsoft, games, net, technology, virus, phone, broadband, computer, phones, spam, mail, firms, use, spyware, online, pc, internet\n","\n","\n","** Which list of words describe the two classes better? Briefly explain your reasoning ** \\\n","By maximizing ùë∑(ùëø_ùíò = ùüè|ùíÄ = ùíö) / ùë∑(ùëø_ùíò = ùüè|ùíÄ ‚â† ùíö), the list focuses on words that have a greater chance of being in one class than another. In this list, the selected words mobile, software, technology, etc., are highly specific to the tech class. These words are directly associated with technology and have a higher chance of appearing in articles from the tech class. \n","Whereas, the \"Top-20 Most Identifiable Words\" list includes words like 'us' and 'also' that are not as specific to a particular class. These words may occur frequently in either class, have a more general usage across many domains, and may not be as descriptive when it comes to class separation.\n","\n","Thus, the \"Top-20 Words Maximizing the Quantity\" list is better for capturing the specific and discriminative words that are more indicative of the class, while the \"Top-20 Most Identifiable Words\" list includes words that have less class specific information or have a general usage across different domains.\n","\n"],"metadata":{"id":"yRyj0oxz9vK5"}},{"cell_type":"markdown","source":["## **Part b - SVM**"],"metadata":{"id":"_VYSQZzBrqNQ"}},{"cell_type":"code","source":[],"metadata":{"id":"SaKM0GVirpEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Part d - NN:**\n","Consider a neural network with the following hyperparameters: the initial weights uniformly drawn in range [0,0.1] with learning rate 0.01.\n","\n","(i) Train a single hidden layer neural network using the hyperparameters on the training \n","dataset, except for the number of hidden units (x) which should vary among 5, 20, and 40. Run the optimization for 100 epochs each time. Namely, the input layer consists of n features x = [x1, ..., xn]T , the hidden layer has x nodes z = [z1, ..., zx]T , and the output \n","layer is a probability distribution y = [y1, y2]T over two classes.\n","\n","(ii) Plot the average training cross-entropy loss as shown below on the y-axis versus the number of hidden units on the x-axis. Explain the effect of numbers of hidden units. \n"],"metadata":{"id":"tV53PQNzrQJh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rexgvAbawTg-"},"outputs":[],"source":["\"\"\"Examine the data\"\"\"\n","import pandas as pd\n","\n","df = pd.read_csv('train.csv')\n","df.head()\n","df.info()\n","df = df.drop_duplicates()\n","df.groupby('Category').describe()"]},{"cell_type":"code","source":["\"\"\"Load data from train.csv and test.csv\"\"\"\n","from sklearn.preprocessing import LabelEncoder\n","\n","df = pd.read_csv('train.csv')\n","df = df.drop_duplicates()\n","df_test = pd.read_csv('test.csv')"],"metadata":{"id":"vEM1jSo6sp9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Preprocess data\"\"\"\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.utils import to_categorical\n","\n","def remove_stop_words(text):\n","    text = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    text = [word for word in text if word.isalpha() and not word in stop_words]\n","    return ' '.join(text)\n","#train.csv preprocessing:\n","x = df.apply(lambda row: remove_stop_words(row['Text']), axis=1)\n","y = df['Category']\n","\n","label_encoder = LabelEncoder() # Encode categories into numeric values\n","y = label_encoder.fit_transform(y)\n","\n","max_words = 20000\n","max_length = 500\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(x)\n","sequences = tokenizer.texts_to_sequences(x)\n","x = pad_sequences(sequences, maxlen=max_length)\n","\n","num_classes = len(df['Category'].unique())\n","y = to_categorical(y, num_classes=num_classes) # Convert y to one-hot encoded format\n","\n","#test.csv preprocessing:\n","x_test = df_test.apply(lambda row: remove_stop_words(row['Text']), axis=1)\n","\n","sequences_test = tokenizer.texts_to_sequences(x_test) # Tokenize and pad sequences\n","x_test = pad_sequences(sequences_test, maxlen=max_length)\n","\n","y_test = df_test['Category']\n","y_test = label_encoder.transform(y_test)\n","y_test = to_categorical(y_test, num_classes=num_classes)"],"metadata":{"id":"HE_qW0dxssPS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.d(i)** Train a single hidden layer neural network using the hyperparameters on the training \n","dataset, except for the number of hidden units (x) which should vary among 5, 20, and 40. Run the optimization for 100 epochs each time. Namely, the input layer consists of n features x = [x1, ..., xn]T , the hidden layer has x nodes z = [z1, ..., zx]T , and the output \n","layer is a probability distribution y = [y1, y2]T over two classes."],"metadata":{"id":"nsidf7yZtWED"}},{"cell_type":"code","source":["\"\"\"Train model with the following hyper parameters:\n","1. initial weights uniformly drawn in range [0,0.1] \n","2. earning rate 0.01\n","3. 100 epoch\n","3. loss function: loss = -y1*log(y_pred1) - y2*log(y_pred2)\"\"\"\n","import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from keras.layers import Embedding\n","from keras.optimizers import Adam\n","from keras.initializers import RandomUniform\n","\n","#Define loss function\n","def custom_loss(y_true, y_pred):\n","    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","    loss = -K.sum(y_true * K.log(y_pred), axis=1)\n","    return loss\n","\n","model = Sequential()\n","learning_rate = 0.01  # Set the desired learning rate = 0.01\n","optimizer = Adam(learning_rate=learning_rate)\n","weight_init = RandomUniform(minval=0, maxval=0.1) #Set weight intervals to [0,0.1]\n","model.add(Embedding(max_words, 32, input_length=max_length))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))#, kernel_initializer=weight_init))  #After asking Thomas, he said it's okay to leave the random weights initialisations on [0,0.1] out\n","model.add(Dense(num_classes, activation='softmax'))\n","model.compile(loss=custom_loss, optimizer=optimizer, metrics=['accuracy'])\n","model.summary()\n","\n","hist = model.fit(x, y, validation_split=0.2, epochs=100, batch_size=100) #epochs=100"],"metadata":{"id":"aX-g3W65swo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Plot training and validation accuracy graph\"\"\"\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","sns.set()\n","\n","acc = hist.history['accuracy']\n","val = hist.history['val_accuracy']\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, '-', label='Training accuracy')\n","plt.plot(epochs, val, ':', label='Validation accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')\n","plt.plot()"],"metadata":{"id":"mtBL5RJps3CF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.d(ii)** Plot the average training cross-entropy loss as shown below on the y-axis versus the number of hidden units on the x-axis. Explain the effect of numbers of hidden units. \n"],"metadata":{"id":"qjcGzeX7tBFm"}},{"cell_type":"code","source":["\"\"\"Plot the average training cross-entropy loss as shown below on the y-axis versus the \n","number of hidden units on the x-axis\"\"\"\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","hidden_units = [5, 20, 40]\n","losses = []\n","\n","for units in hidden_units:\n","    # Retrain the model\n","    model = Sequential()\n","    learning_rate = 0.01  # Set the desired learning rate = 0.01\n","    optimizer = Adam(learning_rate=learning_rate)\n","    #weight_init = RandomUniform(minval=0, maxval=0.1) #Set weight intervals to [0,0.1]\n","    model.add(Embedding(max_words, 32, input_length=max_length))\n","    model.add(Flatten())\n","    model.add(Dense(units, activation='relu'))#, kernel_initializer=weight_init)) #After asking Thomas, he said it's okay to leave the random weights initialisations on [0,0.1] out\n","    model.add(Dense(num_classes, activation='softmax'))\n","    model.compile(loss=custom_loss, optimizer=optimizer, metrics=['accuracy'])\n","    # Train the model\n","    hist = model.fit(x, y, validation_split=0.2, epochs=100, batch_size=50, verbose=0)\n","    \n","    # Compute the average training cross-entropy loss\n","    avg_loss = np.mean(hist.history['loss'])\n","    losses.append(avg_loss)\n","\n","# Plot the results\n","plt.plot(hidden_units, losses, marker='o')\n","plt.xlabel('Number of Hidden Units')\n","plt.ylabel('Average Training Cross-Entropy Loss')\n","plt.title('Effect of Number of Hidden Units on Loss')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"hr7Vob-YtBmm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation of the above graph:**\n","The number of hidden units in a neural network refers to the number of neurons or units in the hidden layer(s) of the network. The average training cross-entropy loss measures how well the network is able to minimize the difference between predicted and actual output during training.\n","\n","The general intuition behind the relationship between the number of hidden units and average training cross-entropy loss is as follows:\n","\n","1. Fewer hidden units: If the network has a small number of hidden units, it may have limited capacity to capture the complexity of the underlying data. In this case, the network may struggle to learn the patterns and relationships in the data, leading to a higher training loss. The model may underfit the data, resulting in lower accuracy and higher loss values.\n","\n","2. Optimal number of hidden units: There is typically an optimal number of hidden units that allows the network to effectively learn the patterns in the data without overfitting. This optimal number can vary depending on the specific problem and dataset. Increasing the number of hidden units beyond this optimal point may not provide significant improvement in performance and can potentially lead to overfitting.\n","\n","3. Too many hidden units: If the network has an excessively large number of hidden units, it may have excessive capacity and may start to memorize the training examples instead of learning the underlying patterns. This can lead to overfitting, where the network performs well on the training data but fails to generalize to unseen data. In such cases, the training loss may be very low, indicating good performance on the training set, but the model may perform poorly on new data.\n","\n","Everytime we re-run the above code, it outputs a different graph, sometimes shows that the average accuracy decreases as number of hidden units increases, soemtimes shows that the average accuracy decreases from increasing 5 hidden units to 20 but increases from 20 to 40 hidden units. And there are other results as well.\n","\n","This depends on several parameters in the trained model, especially when initial weights is up to random in intervals [0, 0.1]."],"metadata":{"id":"2O5mHUwN343z"}},{"cell_type":"markdown","source":["# **Task 3: Classification Quality Evaluation**\n","\n"],"metadata":{"id":"ADH850e59vWs"}},{"cell_type":"markdown","source":["## **Part a:**\n","We explore how the size of the training data set affects the test and train accuracy. For each value of m in [0.1, 0.3, 0.5, 0.7, 0.9], train your classifier on the first m portion of the training examples (that is, use the data given by XTrain[0:mN] and yTrain[0:mN]). \\\n","\\\n","Please report two plots: \\\n","(i) training and \\\n","(ii) testing accuracy for each such value of m with the x-axis referring to m\n","and the y-axis referring to the classification accuracy in ùêπùêπ1 measure as shown below. In total,\n","there should be four curves for training accuracy and four curves for testing accuracy. \\\n","Explain the general trend of the two plots in terms of training and testing accuracy if any. \\\n","\n","ùêπ1 = 2 ‚àô ( (ùëÉrecision √ó ùëÖecall) / (ùëÉrecision + ùëÖecall) ) \\"],"metadata":{"id":"Z-7tZap3wxhj"}},{"cell_type":"markdown","source":["### **1. NB:**"],"metadata":{"id":"IMQNMkUBDAgu"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.model_selection import KFold\n","\n","# Load the training dataset\n","train_data = pd.read_csv('train.csv')\n","\n","# Load the testing dataset\n","test_data = pd.read_csv('test.csv')\n","\n","# Create the feature matrix using TF-IDF vectorization\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train_data['Text'])\n","X_test = vectorizer.transform(test_data['Text'])\n","\n","# Create the target variables\n","y_train = train_data['Category']\n","y_test = test_data['Category']\n","\n","# Define the values of m\n","m_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n","\n","# Initialize lists to store results\n","train_accuracies = []\n","test_accuracies = []\n","\n","# Perform m-based training and evaluation\n","for m in m_values:\n","    # Calculate the number of examples based on m\n","    N = int(X_train.shape[0] * m)\n","    \n","    # Select the first N examples for training\n","    X_train_subset, y_train_subset = X_train[:N], y_train[:N]\n","    \n","    # Train the Naive Bayes classifier\n","    classifier = MultinomialNB()\n","    classifier.fit(X_train_subset, y_train_subset)\n","    \n","    # Predict on training and testing data\n","    y_train_pred = classifier.predict(X_train_subset)\n","    y_test_pred = classifier.predict(X_test)\n","    \n","    # Calculate training and testing accuracy using F1 score\n","    train_accuracy = f1_score(y_train_subset, y_train_pred, average='weighted')\n","    test_accuracy = f1_score(y_test, y_test_pred, average='weighted')\n","    \n","    # Store accuracy values\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    \n","    # Print the evaluation metrics for each m\n","    print(f\"Training Data Siz (m) = {m}\")\n","    print(\"Training Accuracy (F1-score):\", train_accuracy)\n","    print(\"Testing Accuracy (F1-score):\", test_accuracy)\n","    print(\"--------------------------------------\")\n"],"metadata":{"id":"gZqrtunYCoBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot the training and testing accuracy curves\n","plt.plot(m_values, train_accuracies, marker='o', label=\"Training Accuracy\")\n","plt.plot(m_values, test_accuracies, marker='o', label=\"Testing Accuracy\")\n","plt.xlabel(\"Training Data Size (m)\")\n","plt.ylabel(\"Accuracy (F1-score)\")\n","plt.title(\"Training and Testing Accuracy vs. Training Data Size\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"jwAdov52Cz5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["** Explain the general trend of the two plots in terms of training and testing accuracy if any. **\n","\n","The training accuracy is consistently high and close to 1 across all the different sizes of the training data evaluated. As more training data is included, the classifier becomes more capable of capturing the underlying patterns, resulting in higher accuracy.\n","\n","The testing accuracy shows a noticeable difference when evaluated on a very small subset of the training data (m = 0.1). In this case, the testing accuracy is significantly lower than the training accuracy. This suggests that the classifier may be overfitting to the small training subset and doesn't generalize well to new data. However, as the size of the training data increases (m = 0.3, 0.5, 0.7, 0.9), both the training and testing accuracies are high and align closely. \n","\n","The general trend suggests that increasing the size of the training data improves the classifier's ability to generalize and perform well on new data and reduces the risk of overfitting."],"metadata":{"id":"gUgbdEVUCpSg"}},{"cell_type":"markdown","source":["### **4. NN:**"],"metadata":{"id":"O4oEbflyyVja"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","from sklearn.metrics import f1_score\n","import matplotlib.pyplot as plt\n","from keras.optimizers import Adam\n","\n","def remove_stop_words(text):\n","    text = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    text = [word for word in text if word.isalpha() and not word in stop_words]\n","    return ' '.join(text)\n","\n","df = pd.read_csv('train.csv')\n","df = df.drop_duplicates()\n","\n","x = df.apply(lambda row: remove_stop_words(row['Text']), axis=1)\n","y = df['Category']\n","\n","# Encode categories into numeric values\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(y)\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.utils import to_categorical\n","\n","max_words = 20000\n","max_length = 500\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(x)\n","sequences = tokenizer.texts_to_sequences(x)\n","x = pad_sequences(sequences, maxlen=max_length)\n","\n","# Convert y to one-hot encoded format\n","num_classes = len(df['Category'].unique())\n","y = to_categorical(y, num_classes=num_classes)\n","\n","df_test = pd.read_csv('test.csv')\n","\n","x_test = df_test.apply(lambda row: remove_stop_words(row['Text']), axis=1)\n","\n","# Tokenize and pad sequences\n","sequences_test = tokenizer.texts_to_sequences(x_test)\n","x_test = pad_sequences(sequences_test, maxlen=max_length)\n","\n","y_test = df_test['Category']\n","y_test = label_encoder.transform(y_test)\n","y_test = to_categorical(y_test, num_classes=num_classes)\n","\n","m_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n","train_accuracies = []\n","test_accuracies = []\n","\n","for m in m_values:\n","    # Determine the number of training samples based on m\n","    num_samples = int(m * x.shape[0])\n","    x_train_subset = x[:num_samples]\n","    y_train_subset = y[:num_samples]\n","    \n","    # Create and train the model\n","    model = Sequential()\n","    model.add(Embedding(max_words, 32, input_length=max_length))\n","    model.add(Flatten())\n","    learning_rate = 0.01  # Set the desired learning rate = 0.01\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.add(Dense(40, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","    model.compile(loss=custom_loss, optimizer=optimizer, metrics=['accuracy'])\n","    model.fit(x_train_subset, y_train_subset, epochs=100, batch_size=100, verbose=0)\n","    \n","    # Predict on the training and test datasets\n","    y_train_pred = model.predict(x_train_subset)\n","    y_test_pred = model.predict(x_test)\n","    \n","    # Convert one-hot encoded labels to categorical labels\n","    y_train_subset_labels = np.argmax(y_train_subset, axis=1)\n","    y_test_labels = np.argmax(y_test, axis=1)\n","    \n","    # Convert predictions to class labels\n","    y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n","    y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n","    \n","    # Compute accuracy using F1 measure\n","    train_accuracy = f1_score(y_train_subset_labels, y_train_pred_labels, average='micro')\n","    test_accuracy = f1_score(y_test_labels, y_test_pred_labels, average='micro')\n","    \n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","\n","# Plot the training and testing accuracies\n","plt.plot(m_values, train_accuracies, marker='o', label='Training Accuracy')\n","plt.plot(m_values, test_accuracies, marker='o', label='Testing Accuracy')\n","plt.xlabel('Training Data Size (m)')\n","plt.ylabel('Accuracy (F1 Measure)')\n","plt.title('Effect of Training Data Size on Accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"jQX-MSAVygGa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that due to random initial weights, the model generates different results everytime it's reran.\n","\n","In general, as the size of the training dataset (m) increases, we can expect the following trends:\n","\n","**Training Accuracy:**\n","\n","Initially, when the training dataset is small (m=0.1), the model has limited exposure to the data, resulting in lower training accuracy. The model may struggle to capture the underlying patterns and generalize well.\n","\n","As the training dataset size increases, the model has access to more diverse examples and can learn more representative patterns. Consequently, the training accuracy tends to improve.\n","\n","However, there might be a saturation point where further increasing the training dataset size doesn't significantly improve the training accuracy. This could indicate that the model has already captured the essential patterns and additional data doesn't contribute significantly to its performance.\n","\n","**Testing Accuracy:**\n","\n","Similar to training accuracy, when the training dataset is small (m=0.1), the model may have limited exposure to the data, leading to lower testing accuracy. The model might not generalize well to unseen examples.\n","\n","As the training dataset size increases, the model has more examples to learn from and can potentially generalize better to unseen data. This often results in an improvement in testing accuracy.\n","\n","However, it's important to note that if the training dataset becomes too large (m=0.9), there might be a risk of overfitting. Overfitting occurs when the model becomes too specific to the training data and performs poorly on unseen examples. This can lead to a decrease in testing accuracy despite high training accuracy.\n","\n","In summary, the general trend in the two plots of training and testing accuracy as the training dataset size increases is an initial improvement in accuracy as the model gets exposed to more data and learns representative patterns. However, there may be a point of diminishing returns or a risk of overfitting if the dataset size becomes excessively large. It's crucial to strike a balance and find the optimal dataset size that maximizes the model's ability to generalize to unseen examples."],"metadata":{"id":"7IPFLOF78ziS"}},{"cell_type":"markdown","source":["## **Part b** \n","Let‚Äôs use 5-fold cross-validation to assess model performance. Investigate the impact of key\n","hyperparameters of your choices for each classifier using a testing dataset. E.g., for SVM, the\n","classification accuracy may be significantly affected by the kernels and hyperparameter\n","combination. List hyperparameters for each classifier and demonstrate how these\n","hyperparameters impact on the testing accuracy. \\\n"],"metadata":{"id":"lfJ-GOSu9v0O"}},{"cell_type":"markdown","source":["### **1. NB:**"],"metadata":{"id":"E888k2oixf2c"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.model_selection import KFold\n","\n","# Load the training dataset\n","train_data = pd.read_csv('train.csv')\n","\n","# Load the testing dataset\n","test_data = pd.read_csv('test.csv')\n","\n","# Create the feature matrix using TF-IDF vectorization\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train_data['Text'])\n","X_test = vectorizer.transform(test_data['Text'])\n","\n","# Create the target variables\n","y_train = train_data['Category']\n","y_test = test_data['Category']\n","\n","# Define the hyperparameters to investigate\n","alpha_values = [0.001, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0]\n","\n","# Initialize lists to store results\n","train_scores = []\n","val_scores = []\n","test_scores = []\n","\n","# Perform 5-fold cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","for alpha in alpha_values:    \n","    for train_index, val_index in kf.split(X_train):\n","        # Split the data into training and validation folds\n","        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n","        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","        \n","        # Train the Naive Bayes classifier\n","        classifier = MultinomialNB(alpha=alpha)\n","        classifier.fit(X_train_fold, y_train_fold)\n","        \n","        # Predict on the validation fold\n","        y_val_pred = classifier.predict(X_val_fold)\n","        \n","        # Calculate training and validation accuracy\n","        train_acc = accuracy_score(y_train_fold, classifier.predict(X_train_fold))\n","        val_acc = accuracy_score(y_val_fold, classifier.predict(X_val_fold))\n","        \n","        # Store the scores\n","        train_scores.append(train_acc)\n","        val_scores.append(val_acc)\n","    \n","    # Calculate average scores across the folds\n","    avg_train_acc = sum(train_scores) / len(train_scores)\n","    avg_val_acc = sum(val_scores) / len(val_scores)\n","    \n","    # Train the final classifier on the entire training set with the best hyperparameter\n","    final_classifier = MultinomialNB(alpha=alpha)\n","    final_classifier.fit(X_train, y_train)\n","    \n","    # Evaluate on the test set\n","    y_test_pred = final_classifier.predict(X_test)\n","    test_acc = accuracy_score(y_test, y_test_pred)\n","    \n","    # Store the scores\n","    train_scores.append(avg_train_acc)\n","    val_scores.append(avg_val_acc)\n","    test_scores.append(test_acc)\n","    \n","    # Print the results\n","    print(f\"Alpha: {alpha}\")\n","    print(f\"Avg Training Accuracy: {avg_train_acc}\")\n","    print(f\"Avg Validation Accuracy: {avg_val_acc}\")\n","    print(f\"Test Accuracy: {test_acc}\")\n","    print(\"-------------------------------\")\n","\n","# Print the overall performance summary\n","print(\"Overall Performance Summary\")\n","print(f\"Best Alpha: {alpha_values[test_scores.index(max(test_scores))]}\")\n","print(f\"Best Test F1-score: {max(test_scores)}\")\n"],"metadata":{"id":"3eh-gKY5FUs1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot the training and testing accuracies\n","plt.plot(alpha_values, train_scores[:len(alpha_values)], marker='o', label='Training Accuracy')\n","plt.plot(alpha_values, val_scores[:len(alpha_values)], marker='o', label='Validation Accuracy')\n","plt.plot(alpha_values, test_scores, marker='o', label='Testing Accuracy')\n","plt.xlabel('Alpha')\n","plt.ylabel('Accuracy')\n","plt.title('Impact of Hyperparameter Alpha on Accuracy')\n","plt.legend()\n","plt.ylim(0.9, 1.01)\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"llIzYu1IFano"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **4. NN:**"],"metadata":{"id":"hxY5FFAmyr7L"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Define the hyperparameters to explore\n","hidden_units = [5, 40]\n","learning_rates = [0.001, 0.1]\n","dropout_rates = [0.2, 0.6]\n","batch_sizes = [50, 100]\n","\n","# Initialize lists to store results\n","results = []\n","param_combinations = []\n","\n","# Perform cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","for hu in hidden_units:\n","    for lr in learning_rates:\n","        for dr in dropout_rates:\n","            for bs in batch_sizes:\n","                fold_results = []\n","                for train_index, test_index in kf.split(x):\n","                    x_train, x_test = x[train_index], x[test_index]\n","                    y_train, y_test = y[train_index], y[test_index]\n","\n","                    # Create and train the model\n","                    model = Sequential()\n","                    learning_rate = 0.01  # Set the desired learning rate = 0.01\n","                    optimizer = Adam(learning_rate=learning_rate)\n","                    weight_init = RandomUniform(minval=0, maxval=0.1) #Set weight intervals to [0,0.1]\n","                    model.add(Embedding(max_words, 32, input_length=max_length))\n","                    model.add(Flatten())\n","                    model.add(Dense(units, activation='relu'))#, kernel_initializer=weight_init))\n","                    model.add(Dense(num_classes, activation='softmax'))\n","                    model.compile(loss=custom_loss, optimizer=optimizer, metrics=['accuracy'])\n","                    # Train the model\n","                    hist = model.fit(x, y, validation_split=0.2, epochs=100, batch_size=50, verbose=0)\n","\n","                    # Evaluate the model on testing data\n","                    y_pred = model.predict(x_test)\n","                    y_pred_labels = np.argmax(y_pred, axis=1)\n","                    accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred_labels)\n","                    fold_results.append(accuracy)\n","\n","                avg_accuracy = np.mean(fold_results)\n","                results.append(avg_accuracy)\n","                param_combinations.append((hu, lr, dr, bs))\n","\n","# Find the best hyperparameter combination\n","best_index = np.argmax(results)\n","best_params = param_combinations[best_index]\n","best_accuracy = results[best_index]\n","\n","# Print the best hyperparameter combination and its testing accuracy\n","print(\"Best Hyperparameters:\", best_params)\n","print(\"Testing Accuracy:\", best_accuracy)\n","\n","# Plot the results\n","x_ticks = range(len(results))\n","plt.bar(x_ticks, results)\n","plt.xlabel('Hyperparameter Combinations')\n","plt.ylabel('Testing Accuracy')\n","plt.title('Impact of Hyperparameters on Testing Accuracy')\n","plt.xticks(x_ticks, param_combinations, rotation='vertical')\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"XWKc6wM3yu5O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**List of hyperparameters:**\n","\n","hidden_units = [5, 40]\n","\n","learning_rates = [0.001, 0.1]\n","\n","dropout_rates = [0.2, 0.6]\n","\n","batch_sizes = [50, 100]\n","\n","The affects of the above hyperparameters are displayed on the plot above, showing that different combinations of hyperparameters allows different levels of accuracy. However, it's important to note that since the initial weights are randomly assigned, everytime we rerun the model it will churn out a different optimal combinations. \n","\n","However, the general ideas are that:\n","\n","1. Hidden Units: The number of hidden units determines the capacity or complexity of the model. Increasing the number of hidden units can increase the model's ability to capture complex patterns in the data, potentially leading to improved accuracy. However, using too many hidden units can also result in overfitting if the model becomes too complex for the given dataset.\n","\n","2. Learning Rate: The learning rate controls the step size at which the model's weights are updated during training. A higher learning rate can result in faster convergence during training, but it may also cause the model to overshoot the optimal solution and lead to instability or divergence. On the other hand, a lower learning rate may result in slower convergence but can potentially lead to better accuracy if the model is able to find a more precise solution.\n","\n","3. Dropout Rate: Dropout is a regularization technique that randomly drops out a fraction of the neurons during training. It helps prevent overfitting by reducing the interdependence of neurons and promoting better generalization. A higher dropout rate can increase the model's ability to generalize but may also lead to underfitting if too many neurons are dropped. Conversely, a lower dropout rate may result in more overfitting if the model relies heavily on specific neurons.\n","\n","4. Batch Size: The batch size determines the number of samples processed before the model's weights are updated during training. A larger batch size can provide a smoother gradient estimation, leading to more stable updates and potentially faster convergence. However, using a larger batch size requires more memory, and the model may get stuck in suboptimal solutions. Smaller batch sizes can introduce more stochasticity and exploration during training, potentially leading to better accuracy, but they may also result in slower convergence.\n","\n","\n","\n"],"metadata":{"id":"qL8X11sM7ijq"}},{"cell_type":"markdown","source":["## **Part c:**\n","\n","Report and compare your NB, kNN, SVM and NN classifiers with the best hyperparameter\n","settings. Summarize what you have observed in the classification accuracy in ùêπùêπ1 measure on\n","the testing dataset."],"metadata":{"id":"HiZCZjzwCZ3R"}},{"cell_type":"markdown","source":["### **1. NB:**"],"metadata":{"id":"_ULcWhjVVwKw"}},{"cell_type":"markdown","source":["### **2. kNN:**"],"metadata":{"id":"4r9m4_wDV0bO"}},{"cell_type":"markdown","source":["### **3. SVM:**"],"metadata":{"id":"iv83ENKsV3-1"}},{"cell_type":"markdown","source":["### **4. NN:**"],"metadata":{"id":"ZnBxO2bwy7s7"}},{"cell_type":"markdown","source":["# **Task 4: Report Writing**"],"metadata":{"id":"Zr4hFY-89iHL"}}]}